{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGOR gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y= np.array([0,1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.85964222\n",
      "Iteration 2, loss = 0.85209933\n",
      "Iteration 3, loss = 0.84471327\n",
      "Iteration 4, loss = 0.83748871\n",
      "Iteration 5, loss = 0.83043016\n",
      "Iteration 6, loss = 0.82354195\n",
      "Iteration 7, loss = 0.81682821\n",
      "Iteration 8, loss = 0.81029279\n",
      "Iteration 9, loss = 0.80393932\n",
      "Iteration 10, loss = 0.79777112\n",
      "Iteration 11, loss = 0.79179121\n",
      "Iteration 12, loss = 0.78600228\n",
      "Iteration 13, loss = 0.78040669\n",
      "Iteration 14, loss = 0.77500644\n",
      "Iteration 15, loss = 0.76980315\n",
      "Iteration 16, loss = 0.76479804\n",
      "Iteration 17, loss = 0.75999190\n",
      "Iteration 18, loss = 0.75538511\n",
      "Iteration 19, loss = 0.75097758\n",
      "Iteration 20, loss = 0.74676879\n",
      "Iteration 21, loss = 0.74275772\n",
      "Iteration 22, loss = 0.73894292\n",
      "Iteration 23, loss = 0.73532246\n",
      "Iteration 24, loss = 0.73189397\n",
      "Iteration 25, loss = 0.72865461\n",
      "Iteration 26, loss = 0.72560110\n",
      "Iteration 27, loss = 0.72272975\n",
      "Iteration 28, loss = 0.72003646\n",
      "Iteration 29, loss = 0.71751673\n",
      "Iteration 30, loss = 0.71516571\n",
      "Iteration 31, loss = 0.71297821\n",
      "Iteration 32, loss = 0.71094874\n",
      "Iteration 33, loss = 0.70907153\n",
      "Iteration 34, loss = 0.70734056\n",
      "Iteration 35, loss = 0.70574962\n",
      "Iteration 36, loss = 0.70429232\n",
      "Iteration 37, loss = 0.70296213\n",
      "Iteration 38, loss = 0.70175245\n",
      "Iteration 39, loss = 0.70065659\n",
      "Iteration 40, loss = 0.69966786\n",
      "Iteration 41, loss = 0.69877960\n",
      "Iteration 42, loss = 0.69798517\n",
      "Iteration 43, loss = 0.69727805\n",
      "Iteration 44, loss = 0.69665182\n",
      "Iteration 45, loss = 0.69610022\n",
      "Iteration 46, loss = 0.69561716\n",
      "Iteration 47, loss = 0.69519676\n",
      "Iteration 48, loss = 0.69483337\n",
      "Iteration 49, loss = 0.69452157\n",
      "Iteration 50, loss = 0.69425621\n",
      "Iteration 51, loss = 0.69403242\n",
      "Iteration 52, loss = 0.69384561\n",
      "Iteration 53, loss = 0.69369149\n",
      "Iteration 54, loss = 0.69356604\n",
      "Iteration 55, loss = 0.69346557\n",
      "Iteration 56, loss = 0.69338667\n",
      "Iteration 57, loss = 0.69332621\n",
      "Iteration 58, loss = 0.69328135\n",
      "Iteration 59, loss = 0.69324955\n",
      "Iteration 60, loss = 0.69322851\n",
      "Iteration 61, loss = 0.69321618\n",
      "Iteration 62, loss = 0.69321077\n",
      "Iteration 63, loss = 0.69321072\n",
      "Iteration 64, loss = 0.69321466\n",
      "Iteration 65, loss = 0.69322142\n",
      "Iteration 66, loss = 0.69323003\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
       "              beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(50,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=10, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLPClassifier(hidden_layer_sizes = (50 , ) , activation = 'logistic' , solver = 'adam' , max_iter = 1000 , verbose = 10)\n",
    "model.fit(X , Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "predict = model.predict(X)\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    }
   ],
   "source": [
    "print(model.score(X , Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AND Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y= np.array([0,0,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.74406040\n",
      "Iteration 2, loss = 0.73611520\n",
      "Iteration 3, loss = 0.72837791\n",
      "Iteration 4, loss = 0.72085223\n",
      "Iteration 5, loss = 0.71354120\n",
      "Iteration 6, loss = 0.70644703\n",
      "Iteration 7, loss = 0.69957075\n",
      "Iteration 8, loss = 0.69291195\n",
      "Iteration 9, loss = 0.68646845\n",
      "Iteration 10, loss = 0.68023644\n",
      "Iteration 11, loss = 0.67421102\n",
      "Iteration 12, loss = 0.66838708\n",
      "Iteration 13, loss = 0.66276005\n",
      "Iteration 14, loss = 0.65732612\n",
      "Iteration 15, loss = 0.65208212\n",
      "Iteration 16, loss = 0.64702528\n",
      "Iteration 17, loss = 0.64215296\n",
      "Iteration 18, loss = 0.63746250\n",
      "Iteration 19, loss = 0.63295112\n",
      "Iteration 20, loss = 0.62861582\n",
      "Iteration 21, loss = 0.62445343\n",
      "Iteration 22, loss = 0.62046053\n",
      "Iteration 23, loss = 0.61663350\n",
      "Iteration 24, loss = 0.61296851\n",
      "Iteration 25, loss = 0.60946152\n",
      "Iteration 26, loss = 0.60610834\n",
      "Iteration 27, loss = 0.60290460\n",
      "Iteration 28, loss = 0.59984583\n",
      "Iteration 29, loss = 0.59692739\n",
      "Iteration 30, loss = 0.59414460\n",
      "Iteration 31, loss = 0.59149266\n",
      "Iteration 32, loss = 0.58896674\n",
      "Iteration 33, loss = 0.58656198\n",
      "Iteration 34, loss = 0.58427348\n",
      "Iteration 35, loss = 0.58209637\n",
      "Iteration 36, loss = 0.58002578\n",
      "Iteration 37, loss = 0.57805689\n",
      "Iteration 38, loss = 0.57618494\n",
      "Iteration 39, loss = 0.57440522\n",
      "Iteration 40, loss = 0.57271311\n",
      "Iteration 41, loss = 0.57110408\n",
      "Iteration 42, loss = 0.56957373\n",
      "Iteration 43, loss = 0.56811773\n",
      "Iteration 44, loss = 0.56673191\n",
      "Iteration 45, loss = 0.56541222\n",
      "Iteration 46, loss = 0.56415476\n",
      "Iteration 47, loss = 0.56295575\n",
      "Iteration 48, loss = 0.56181156\n",
      "Iteration 49, loss = 0.56071874\n",
      "Iteration 50, loss = 0.55967395\n",
      "Iteration 51, loss = 0.55867402\n",
      "Iteration 52, loss = 0.55771594\n",
      "Iteration 53, loss = 0.55679682\n",
      "Iteration 54, loss = 0.55591396\n",
      "Iteration 55, loss = 0.55506477\n",
      "Iteration 56, loss = 0.55424681\n",
      "Iteration 57, loss = 0.55345780\n",
      "Iteration 58, loss = 0.55269558\n",
      "Iteration 59, loss = 0.55195812\n",
      "Iteration 60, loss = 0.55124352\n",
      "Iteration 61, loss = 0.55055000\n",
      "Iteration 62, loss = 0.54987591\n",
      "Iteration 63, loss = 0.54921971\n",
      "Iteration 64, loss = 0.54857995\n",
      "Iteration 65, loss = 0.54795531\n",
      "Iteration 66, loss = 0.54734454\n",
      "Iteration 67, loss = 0.54674651\n",
      "Iteration 68, loss = 0.54616014\n",
      "Iteration 69, loss = 0.54558448\n",
      "Iteration 70, loss = 0.54501862\n",
      "Iteration 71, loss = 0.54446173\n",
      "Iteration 72, loss = 0.54391305\n",
      "Iteration 73, loss = 0.54337190\n",
      "Iteration 74, loss = 0.54283762\n",
      "Iteration 75, loss = 0.54230965\n",
      "Iteration 76, loss = 0.54178745\n",
      "Iteration 77, loss = 0.54127052\n",
      "Iteration 78, loss = 0.54075844\n",
      "Iteration 79, loss = 0.54025079\n",
      "Iteration 80, loss = 0.53974721\n",
      "Iteration 81, loss = 0.53924736\n",
      "Iteration 82, loss = 0.53875094\n",
      "Iteration 83, loss = 0.53825768\n",
      "Iteration 84, loss = 0.53776732\n",
      "Iteration 85, loss = 0.53727963\n",
      "Iteration 86, loss = 0.53679441\n",
      "Iteration 87, loss = 0.53631146\n",
      "Iteration 88, loss = 0.53583061\n",
      "Iteration 89, loss = 0.53535172\n",
      "Iteration 90, loss = 0.53487462\n",
      "Iteration 91, loss = 0.53439919\n",
      "Iteration 92, loss = 0.53392532\n",
      "Iteration 93, loss = 0.53345288\n",
      "Iteration 94, loss = 0.53298177\n",
      "Iteration 95, loss = 0.53251190\n",
      "Iteration 96, loss = 0.53204319\n",
      "Iteration 97, loss = 0.53157554\n",
      "Iteration 98, loss = 0.53110888\n",
      "Iteration 99, loss = 0.53064314\n",
      "Iteration 100, loss = 0.53017825\n",
      "Iteration 101, loss = 0.52971414\n",
      "Iteration 102, loss = 0.52925076\n",
      "Iteration 103, loss = 0.52878804\n",
      "Iteration 104, loss = 0.52832593\n",
      "Iteration 105, loss = 0.52786438\n",
      "Iteration 106, loss = 0.52740333\n",
      "Iteration 107, loss = 0.52694273\n",
      "Iteration 108, loss = 0.52648254\n",
      "Iteration 109, loss = 0.52602270\n",
      "Iteration 110, loss = 0.52556317\n",
      "Iteration 111, loss = 0.52510392\n",
      "Iteration 112, loss = 0.52464488\n",
      "Iteration 113, loss = 0.52418603\n",
      "Iteration 114, loss = 0.52372732\n",
      "Iteration 115, loss = 0.52326871\n",
      "Iteration 116, loss = 0.52281016\n",
      "Iteration 117, loss = 0.52235163\n",
      "Iteration 118, loss = 0.52189308\n",
      "Iteration 119, loss = 0.52143448\n",
      "Iteration 120, loss = 0.52097579\n",
      "Iteration 121, loss = 0.52051698\n",
      "Iteration 122, loss = 0.52005800\n",
      "Iteration 123, loss = 0.51959883\n",
      "Iteration 124, loss = 0.51913943\n",
      "Iteration 125, loss = 0.51867976\n",
      "Iteration 126, loss = 0.51821981\n",
      "Iteration 127, loss = 0.51775952\n",
      "Iteration 128, loss = 0.51729888\n",
      "Iteration 129, loss = 0.51683785\n",
      "Iteration 130, loss = 0.51637640\n",
      "Iteration 131, loss = 0.51591450\n",
      "Iteration 132, loss = 0.51545213\n",
      "Iteration 133, loss = 0.51498925\n",
      "Iteration 134, loss = 0.51452584\n",
      "Iteration 135, loss = 0.51406187\n",
      "Iteration 136, loss = 0.51359731\n",
      "Iteration 137, loss = 0.51313214\n",
      "Iteration 138, loss = 0.51266633\n",
      "Iteration 139, loss = 0.51219985\n",
      "Iteration 140, loss = 0.51173269\n",
      "Iteration 141, loss = 0.51126481\n",
      "Iteration 142, loss = 0.51079619\n",
      "Iteration 143, loss = 0.51032681\n",
      "Iteration 144, loss = 0.50985665\n",
      "Iteration 145, loss = 0.50938568\n",
      "Iteration 146, loss = 0.50891387\n",
      "Iteration 147, loss = 0.50844122\n",
      "Iteration 148, loss = 0.50796769\n",
      "Iteration 149, loss = 0.50749326\n",
      "Iteration 150, loss = 0.50701792\n",
      "Iteration 151, loss = 0.50654164\n",
      "Iteration 152, loss = 0.50606440\n",
      "Iteration 153, loss = 0.50558618\n",
      "Iteration 154, loss = 0.50510696\n",
      "Iteration 155, loss = 0.50462673\n",
      "Iteration 156, loss = 0.50414546\n",
      "Iteration 157, loss = 0.50366313\n",
      "Iteration 158, loss = 0.50317972\n",
      "Iteration 159, loss = 0.50269523\n",
      "Iteration 160, loss = 0.50220962\n",
      "Iteration 161, loss = 0.50172288\n",
      "Iteration 162, loss = 0.50123499\n",
      "Iteration 163, loss = 0.50074594\n",
      "Iteration 164, loss = 0.50025570\n",
      "Iteration 165, loss = 0.49976427\n",
      "Iteration 166, loss = 0.49927162\n",
      "Iteration 167, loss = 0.49877773\n",
      "Iteration 168, loss = 0.49828260\n",
      "Iteration 169, loss = 0.49778620\n",
      "Iteration 170, loss = 0.49728852\n",
      "Iteration 171, loss = 0.49678955\n",
      "Iteration 172, loss = 0.49628926\n",
      "Iteration 173, loss = 0.49578764\n",
      "Iteration 174, loss = 0.49528468\n",
      "Iteration 175, loss = 0.49478036\n",
      "Iteration 176, loss = 0.49427468\n",
      "Iteration 177, loss = 0.49376760\n",
      "Iteration 178, loss = 0.49325912\n",
      "Iteration 179, loss = 0.49274923\n",
      "Iteration 180, loss = 0.49223792\n",
      "Iteration 181, loss = 0.49172516\n",
      "Iteration 182, loss = 0.49121094\n",
      "Iteration 183, loss = 0.49069526\n",
      "Iteration 184, loss = 0.49017810\n",
      "Iteration 185, loss = 0.48965944\n",
      "Iteration 186, loss = 0.48913928\n",
      "Iteration 187, loss = 0.48861759\n",
      "Iteration 188, loss = 0.48809438\n",
      "Iteration 189, loss = 0.48756963\n",
      "Iteration 190, loss = 0.48704332\n",
      "Iteration 191, loss = 0.48651544\n",
      "Iteration 192, loss = 0.48598599\n",
      "Iteration 193, loss = 0.48545496\n",
      "Iteration 194, loss = 0.48492232\n",
      "Iteration 195, loss = 0.48438807\n",
      "Iteration 196, loss = 0.48385221\n",
      "Iteration 197, loss = 0.48331471\n",
      "Iteration 198, loss = 0.48277557\n",
      "Iteration 199, loss = 0.48223479\n",
      "Iteration 200, loss = 0.48169234\n",
      "Iteration 201, loss = 0.48114823\n",
      "Iteration 202, loss = 0.48060244\n",
      "Iteration 203, loss = 0.48005496\n",
      "Iteration 204, loss = 0.47950578\n",
      "Iteration 205, loss = 0.47895491\n",
      "Iteration 206, loss = 0.47840232\n",
      "Iteration 207, loss = 0.47784800\n",
      "Iteration 208, loss = 0.47729196\n",
      "Iteration 209, loss = 0.47673419\n",
      "Iteration 210, loss = 0.47617466\n",
      "Iteration 211, loss = 0.47561339\n",
      "Iteration 212, loss = 0.47505036\n",
      "Iteration 213, loss = 0.47448556\n",
      "Iteration 214, loss = 0.47391899\n",
      "Iteration 215, loss = 0.47335065\n",
      "Iteration 216, loss = 0.47278051\n",
      "Iteration 217, loss = 0.47220859\n",
      "Iteration 218, loss = 0.47163487\n",
      "Iteration 219, loss = 0.47105935\n",
      "Iteration 220, loss = 0.47048202\n",
      "Iteration 221, loss = 0.46990288\n",
      "Iteration 222, loss = 0.46932192\n",
      "Iteration 223, loss = 0.46873913\n",
      "Iteration 224, loss = 0.46815452\n",
      "Iteration 225, loss = 0.46756808\n",
      "Iteration 226, loss = 0.46697980\n",
      "Iteration 227, loss = 0.46638968\n",
      "Iteration 228, loss = 0.46579772\n",
      "Iteration 229, loss = 0.46520391\n",
      "Iteration 230, loss = 0.46460825\n",
      "Iteration 231, loss = 0.46401074\n",
      "Iteration 232, loss = 0.46341137\n",
      "Iteration 233, loss = 0.46281015\n",
      "Iteration 234, loss = 0.46220706\n",
      "Iteration 235, loss = 0.46160211\n",
      "Iteration 236, loss = 0.46099530\n",
      "Iteration 237, loss = 0.46038662\n",
      "Iteration 238, loss = 0.45977607\n",
      "Iteration 239, loss = 0.45916365\n",
      "Iteration 240, loss = 0.45854936\n",
      "Iteration 241, loss = 0.45793320\n",
      "Iteration 242, loss = 0.45731517\n",
      "Iteration 243, loss = 0.45669526\n",
      "Iteration 244, loss = 0.45607349\n",
      "Iteration 245, loss = 0.45544984\n",
      "Iteration 246, loss = 0.45482432\n",
      "Iteration 247, loss = 0.45419693\n",
      "Iteration 248, loss = 0.45356767\n",
      "Iteration 249, loss = 0.45293654\n",
      "Iteration 250, loss = 0.45230354\n",
      "Iteration 251, loss = 0.45166867\n",
      "Iteration 252, loss = 0.45103194\n",
      "Iteration 253, loss = 0.45039335\n",
      "Iteration 254, loss = 0.44975289\n",
      "Iteration 255, loss = 0.44911058\n",
      "Iteration 256, loss = 0.44846641\n",
      "Iteration 257, loss = 0.44782039\n",
      "Iteration 258, loss = 0.44717252\n",
      "Iteration 259, loss = 0.44652280\n",
      "Iteration 260, loss = 0.44587123\n",
      "Iteration 261, loss = 0.44521783\n",
      "Iteration 262, loss = 0.44456259\n",
      "Iteration 263, loss = 0.44390552\n",
      "Iteration 264, loss = 0.44324663\n",
      "Iteration 265, loss = 0.44258591\n",
      "Iteration 266, loss = 0.44192337\n",
      "Iteration 267, loss = 0.44125902\n",
      "Iteration 268, loss = 0.44059286\n",
      "Iteration 269, loss = 0.43992490\n",
      "Iteration 270, loss = 0.43925514\n",
      "Iteration 271, loss = 0.43858360\n",
      "Iteration 272, loss = 0.43791027\n",
      "Iteration 273, loss = 0.43723516\n",
      "Iteration 274, loss = 0.43655828\n",
      "Iteration 275, loss = 0.43587963\n",
      "Iteration 276, loss = 0.43519923\n",
      "Iteration 277, loss = 0.43451708\n",
      "Iteration 278, loss = 0.43383318\n",
      "Iteration 279, loss = 0.43314755\n",
      "Iteration 280, loss = 0.43246020\n",
      "Iteration 281, loss = 0.43177112\n",
      "Iteration 282, loss = 0.43108033\n",
      "Iteration 283, loss = 0.43038784\n",
      "Iteration 284, loss = 0.42969366\n",
      "Iteration 285, loss = 0.42899779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 286, loss = 0.42830025\n",
      "Iteration 287, loss = 0.42760104\n",
      "Iteration 288, loss = 0.42690018\n",
      "Iteration 289, loss = 0.42619766\n",
      "Iteration 290, loss = 0.42549351\n",
      "Iteration 291, loss = 0.42478774\n",
      "Iteration 292, loss = 0.42408034\n",
      "Iteration 293, loss = 0.42337134\n",
      "Iteration 294, loss = 0.42266075\n",
      "Iteration 295, loss = 0.42194857\n",
      "Iteration 296, loss = 0.42123481\n",
      "Iteration 297, loss = 0.42051950\n",
      "Iteration 298, loss = 0.41980263\n",
      "Iteration 299, loss = 0.41908422\n",
      "Iteration 300, loss = 0.41836429\n",
      "Iteration 301, loss = 0.41764284\n",
      "Iteration 302, loss = 0.41691989\n",
      "Iteration 303, loss = 0.41619544\n",
      "Iteration 304, loss = 0.41546952\n",
      "Iteration 305, loss = 0.41474213\n",
      "Iteration 306, loss = 0.41401329\n",
      "Iteration 307, loss = 0.41328301\n",
      "Iteration 308, loss = 0.41255131\n",
      "Iteration 309, loss = 0.41181819\n",
      "Iteration 310, loss = 0.41108366\n",
      "Iteration 311, loss = 0.41034776\n",
      "Iteration 312, loss = 0.40961048\n",
      "Iteration 313, loss = 0.40887184\n",
      "Iteration 314, loss = 0.40813186\n",
      "Iteration 315, loss = 0.40739055\n",
      "Iteration 316, loss = 0.40664792\n",
      "Iteration 317, loss = 0.40590399\n",
      "Iteration 318, loss = 0.40515877\n",
      "Iteration 319, loss = 0.40441228\n",
      "Iteration 320, loss = 0.40366454\n",
      "Iteration 321, loss = 0.40291555\n",
      "Iteration 322, loss = 0.40216534\n",
      "Iteration 323, loss = 0.40141391\n",
      "Iteration 324, loss = 0.40066129\n",
      "Iteration 325, loss = 0.39990749\n",
      "Iteration 326, loss = 0.39915253\n",
      "Iteration 327, loss = 0.39839641\n",
      "Iteration 328, loss = 0.39763917\n",
      "Iteration 329, loss = 0.39688080\n",
      "Iteration 330, loss = 0.39612134\n",
      "Iteration 331, loss = 0.39536079\n",
      "Iteration 332, loss = 0.39459917\n",
      "Iteration 333, loss = 0.39383650\n",
      "Iteration 334, loss = 0.39307280\n",
      "Iteration 335, loss = 0.39230808\n",
      "Iteration 336, loss = 0.39154235\n",
      "Iteration 337, loss = 0.39077564\n",
      "Iteration 338, loss = 0.39000796\n",
      "Iteration 339, loss = 0.38923933\n",
      "Iteration 340, loss = 0.38846977\n",
      "Iteration 341, loss = 0.38769929\n",
      "Iteration 342, loss = 0.38692791\n",
      "Iteration 343, loss = 0.38615564\n",
      "Iteration 344, loss = 0.38538251\n",
      "Iteration 345, loss = 0.38460853\n",
      "Iteration 346, loss = 0.38383373\n",
      "Iteration 347, loss = 0.38305810\n",
      "Iteration 348, loss = 0.38228169\n",
      "Iteration 349, loss = 0.38150449\n",
      "Iteration 350, loss = 0.38072653\n",
      "Iteration 351, loss = 0.37994783\n",
      "Iteration 352, loss = 0.37916841\n",
      "Iteration 353, loss = 0.37838828\n",
      "Iteration 354, loss = 0.37760745\n",
      "Iteration 355, loss = 0.37682596\n",
      "Iteration 356, loss = 0.37604381\n",
      "Iteration 357, loss = 0.37526103\n",
      "Iteration 358, loss = 0.37447763\n",
      "Iteration 359, loss = 0.37369362\n",
      "Iteration 360, loss = 0.37290904\n",
      "Iteration 361, loss = 0.37212389\n",
      "Iteration 362, loss = 0.37133819\n",
      "Iteration 363, loss = 0.37055197\n",
      "Iteration 364, loss = 0.36976523\n",
      "Iteration 365, loss = 0.36897800\n",
      "Iteration 366, loss = 0.36819030\n",
      "Iteration 367, loss = 0.36740214\n",
      "Iteration 368, loss = 0.36661354\n",
      "Iteration 369, loss = 0.36582452\n",
      "Iteration 370, loss = 0.36503510\n",
      "Iteration 371, loss = 0.36424529\n",
      "Iteration 372, loss = 0.36345511\n",
      "Iteration 373, loss = 0.36266459\n",
      "Iteration 374, loss = 0.36187373\n",
      "Iteration 375, loss = 0.36108256\n",
      "Iteration 376, loss = 0.36029110\n",
      "Iteration 377, loss = 0.35949936\n",
      "Iteration 378, loss = 0.35870735\n",
      "Iteration 379, loss = 0.35791511\n",
      "Iteration 380, loss = 0.35712264\n",
      "Iteration 381, loss = 0.35632996\n",
      "Iteration 382, loss = 0.35553710\n",
      "Iteration 383, loss = 0.35474406\n",
      "Iteration 384, loss = 0.35395087\n",
      "Iteration 385, loss = 0.35315754\n",
      "Iteration 386, loss = 0.35236409\n",
      "Iteration 387, loss = 0.35157054\n",
      "Iteration 388, loss = 0.35077691\n",
      "Iteration 389, loss = 0.34998320\n",
      "Iteration 390, loss = 0.34918945\n",
      "Iteration 391, loss = 0.34839567\n",
      "Iteration 392, loss = 0.34760187\n",
      "Iteration 393, loss = 0.34680807\n",
      "Iteration 394, loss = 0.34601428\n",
      "Iteration 395, loss = 0.34522054\n",
      "Iteration 396, loss = 0.34442684\n",
      "Iteration 397, loss = 0.34363321\n",
      "Iteration 398, loss = 0.34283967\n",
      "Iteration 399, loss = 0.34204623\n",
      "Iteration 400, loss = 0.34125291\n",
      "Iteration 401, loss = 0.34045972\n",
      "Iteration 402, loss = 0.33966669\n",
      "Iteration 403, loss = 0.33887382\n",
      "Iteration 404, loss = 0.33808114\n",
      "Iteration 405, loss = 0.33728865\n",
      "Iteration 406, loss = 0.33649638\n",
      "Iteration 407, loss = 0.33570435\n",
      "Iteration 408, loss = 0.33491256\n",
      "Iteration 409, loss = 0.33412103\n",
      "Iteration 410, loss = 0.33332978\n",
      "Iteration 411, loss = 0.33253883\n",
      "Iteration 412, loss = 0.33174818\n",
      "Iteration 413, loss = 0.33095787\n",
      "Iteration 414, loss = 0.33016789\n",
      "Iteration 415, loss = 0.32937827\n",
      "Iteration 416, loss = 0.32858902\n",
      "Iteration 417, loss = 0.32780015\n",
      "Iteration 418, loss = 0.32701169\n",
      "Iteration 419, loss = 0.32622364\n",
      "Iteration 420, loss = 0.32543602\n",
      "Iteration 421, loss = 0.32464885\n",
      "Iteration 422, loss = 0.32386214\n",
      "Iteration 423, loss = 0.32307590\n",
      "Iteration 424, loss = 0.32229014\n",
      "Iteration 425, loss = 0.32150489\n",
      "Iteration 426, loss = 0.32072016\n",
      "Iteration 427, loss = 0.31993595\n",
      "Iteration 428, loss = 0.31915229\n",
      "Iteration 429, loss = 0.31836919\n",
      "Iteration 430, loss = 0.31758666\n",
      "Iteration 431, loss = 0.31680471\n",
      "Iteration 432, loss = 0.31602336\n",
      "Iteration 433, loss = 0.31524262\n",
      "Iteration 434, loss = 0.31446250\n",
      "Iteration 435, loss = 0.31368303\n",
      "Iteration 436, loss = 0.31290420\n",
      "Iteration 437, loss = 0.31212603\n",
      "Iteration 438, loss = 0.31134854\n",
      "Iteration 439, loss = 0.31057174\n",
      "Iteration 440, loss = 0.30979564\n",
      "Iteration 441, loss = 0.30902025\n",
      "Iteration 442, loss = 0.30824559\n",
      "Iteration 443, loss = 0.30747166\n",
      "Iteration 444, loss = 0.30669849\n",
      "Iteration 445, loss = 0.30592607\n",
      "Iteration 446, loss = 0.30515443\n",
      "Iteration 447, loss = 0.30438357\n",
      "Iteration 448, loss = 0.30361350\n",
      "Iteration 449, loss = 0.30284425\n",
      "Iteration 450, loss = 0.30207581\n",
      "Iteration 451, loss = 0.30130820\n",
      "Iteration 452, loss = 0.30054143\n",
      "Iteration 453, loss = 0.29977551\n",
      "Iteration 454, loss = 0.29901046\n",
      "Iteration 455, loss = 0.29824628\n",
      "Iteration 456, loss = 0.29748298\n",
      "Iteration 457, loss = 0.29672057\n",
      "Iteration 458, loss = 0.29595907\n",
      "Iteration 459, loss = 0.29519848\n",
      "Iteration 460, loss = 0.29443882\n",
      "Iteration 461, loss = 0.29368009\n",
      "Iteration 462, loss = 0.29292230\n",
      "Iteration 463, loss = 0.29216547\n",
      "Iteration 464, loss = 0.29140960\n",
      "Iteration 465, loss = 0.29065471\n",
      "Iteration 466, loss = 0.28990079\n",
      "Iteration 467, loss = 0.28914787\n",
      "Iteration 468, loss = 0.28839595\n",
      "Iteration 469, loss = 0.28764504\n",
      "Iteration 470, loss = 0.28689514\n",
      "Iteration 471, loss = 0.28614627\n",
      "Iteration 472, loss = 0.28539844\n",
      "Iteration 473, loss = 0.28465165\n",
      "Iteration 474, loss = 0.28390592\n",
      "Iteration 475, loss = 0.28316125\n",
      "Iteration 476, loss = 0.28241764\n",
      "Iteration 477, loss = 0.28167512\n",
      "Iteration 478, loss = 0.28093367\n",
      "Iteration 479, loss = 0.28019333\n",
      "Iteration 480, loss = 0.27945408\n",
      "Iteration 481, loss = 0.27871594\n",
      "Iteration 482, loss = 0.27797892\n",
      "Iteration 483, loss = 0.27724302\n",
      "Iteration 484, loss = 0.27650825\n",
      "Iteration 485, loss = 0.27577462\n",
      "Iteration 486, loss = 0.27504214\n",
      "Iteration 487, loss = 0.27431080\n",
      "Iteration 488, loss = 0.27358063\n",
      "Iteration 489, loss = 0.27285162\n",
      "Iteration 490, loss = 0.27212379\n",
      "Iteration 491, loss = 0.27139713\n",
      "Iteration 492, loss = 0.27067167\n",
      "Iteration 493, loss = 0.26994739\n",
      "Iteration 494, loss = 0.26922431\n",
      "Iteration 495, loss = 0.26850244\n",
      "Iteration 496, loss = 0.26778177\n",
      "Iteration 497, loss = 0.26706233\n",
      "Iteration 498, loss = 0.26634410\n",
      "Iteration 499, loss = 0.26562711\n",
      "Iteration 500, loss = 0.26491134\n",
      "Iteration 501, loss = 0.26419682\n",
      "Iteration 502, loss = 0.26348355\n",
      "Iteration 503, loss = 0.26277152\n",
      "Iteration 504, loss = 0.26206075\n",
      "Iteration 505, loss = 0.26135124\n",
      "Iteration 506, loss = 0.26064300\n",
      "Iteration 507, loss = 0.25993602\n",
      "Iteration 508, loss = 0.25923033\n",
      "Iteration 509, loss = 0.25852591\n",
      "Iteration 510, loss = 0.25782278\n",
      "Iteration 511, loss = 0.25712094\n",
      "Iteration 512, loss = 0.25642040\n",
      "Iteration 513, loss = 0.25572116\n",
      "Iteration 514, loss = 0.25502322\n",
      "Iteration 515, loss = 0.25432658\n",
      "Iteration 516, loss = 0.25363126\n",
      "Iteration 517, loss = 0.25293726\n",
      "Iteration 518, loss = 0.25224458\n",
      "Iteration 519, loss = 0.25155322\n",
      "Iteration 520, loss = 0.25086319\n",
      "Iteration 521, loss = 0.25017450\n",
      "Iteration 522, loss = 0.24948713\n",
      "Iteration 523, loss = 0.24880111\n",
      "Iteration 524, loss = 0.24811644\n",
      "Iteration 525, loss = 0.24743311\n",
      "Iteration 526, loss = 0.24675113\n",
      "Iteration 527, loss = 0.24607050\n",
      "Iteration 528, loss = 0.24539123\n",
      "Iteration 529, loss = 0.24471332\n",
      "Iteration 530, loss = 0.24403677\n",
      "Iteration 531, loss = 0.24336159\n",
      "Iteration 532, loss = 0.24268778\n",
      "Iteration 533, loss = 0.24201535\n",
      "Iteration 534, loss = 0.24134428\n",
      "Iteration 535, loss = 0.24067460\n",
      "Iteration 536, loss = 0.24000629\n",
      "Iteration 537, loss = 0.23933937\n",
      "Iteration 538, loss = 0.23867384\n",
      "Iteration 539, loss = 0.23800969\n",
      "Iteration 540, loss = 0.23734693\n",
      "Iteration 541, loss = 0.23668557\n",
      "Iteration 542, loss = 0.23602560\n",
      "Iteration 543, loss = 0.23536703\n",
      "Iteration 544, loss = 0.23470986\n",
      "Iteration 545, loss = 0.23405409\n",
      "Iteration 546, loss = 0.23339973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 547, loss = 0.23274677\n",
      "Iteration 548, loss = 0.23209522\n",
      "Iteration 549, loss = 0.23144508\n",
      "Iteration 550, loss = 0.23079635\n",
      "Iteration 551, loss = 0.23014903\n",
      "Iteration 552, loss = 0.22950313\n",
      "Iteration 553, loss = 0.22885864\n",
      "Iteration 554, loss = 0.22821558\n",
      "Iteration 555, loss = 0.22757393\n",
      "Iteration 556, loss = 0.22693370\n",
      "Iteration 557, loss = 0.22629490\n",
      "Iteration 558, loss = 0.22565752\n",
      "Iteration 559, loss = 0.22502157\n",
      "Iteration 560, loss = 0.22438704\n",
      "Iteration 561, loss = 0.22375394\n",
      "Iteration 562, loss = 0.22312227\n",
      "Iteration 563, loss = 0.22249203\n",
      "Iteration 564, loss = 0.22186322\n",
      "Iteration 565, loss = 0.22123584\n",
      "Iteration 566, loss = 0.22060990\n",
      "Iteration 567, loss = 0.21998539\n",
      "Iteration 568, loss = 0.21936231\n",
      "Iteration 569, loss = 0.21874067\n",
      "Iteration 570, loss = 0.21812047\n",
      "Iteration 571, loss = 0.21750170\n",
      "Iteration 572, loss = 0.21688437\n",
      "Iteration 573, loss = 0.21626848\n",
      "Iteration 574, loss = 0.21565403\n",
      "Iteration 575, loss = 0.21504102\n",
      "Iteration 576, loss = 0.21442944\n",
      "Iteration 577, loss = 0.21381931\n",
      "Iteration 578, loss = 0.21321062\n",
      "Iteration 579, loss = 0.21260337\n",
      "Iteration 580, loss = 0.21199756\n",
      "Iteration 581, loss = 0.21139320\n",
      "Iteration 582, loss = 0.21079027\n",
      "Iteration 583, loss = 0.21018879\n",
      "Iteration 584, loss = 0.20958875\n",
      "Iteration 585, loss = 0.20899015\n",
      "Iteration 586, loss = 0.20839300\n",
      "Iteration 587, loss = 0.20779729\n",
      "Iteration 588, loss = 0.20720302\n",
      "Iteration 589, loss = 0.20661019\n",
      "Iteration 590, loss = 0.20601881\n",
      "Iteration 591, loss = 0.20542887\n",
      "Iteration 592, loss = 0.20484037\n",
      "Iteration 593, loss = 0.20425331\n",
      "Iteration 594, loss = 0.20366770\n",
      "Iteration 595, loss = 0.20308353\n",
      "Iteration 596, loss = 0.20250080\n",
      "Iteration 597, loss = 0.20191951\n",
      "Iteration 598, loss = 0.20133966\n",
      "Iteration 599, loss = 0.20076126\n",
      "Iteration 600, loss = 0.20018429\n",
      "Iteration 601, loss = 0.19960876\n",
      "Iteration 602, loss = 0.19903467\n",
      "Iteration 603, loss = 0.19846203\n",
      "Iteration 604, loss = 0.19789082\n",
      "Iteration 605, loss = 0.19732104\n",
      "Iteration 606, loss = 0.19675271\n",
      "Iteration 607, loss = 0.19618581\n",
      "Iteration 608, loss = 0.19562035\n",
      "Iteration 609, loss = 0.19505632\n",
      "Iteration 610, loss = 0.19449373\n",
      "Iteration 611, loss = 0.19393257\n",
      "Iteration 612, loss = 0.19337285\n",
      "Iteration 613, loss = 0.19281456\n",
      "Iteration 614, loss = 0.19225770\n",
      "Iteration 615, loss = 0.19170227\n",
      "Iteration 616, loss = 0.19114827\n",
      "Iteration 617, loss = 0.19059570\n",
      "Iteration 618, loss = 0.19004456\n",
      "Iteration 619, loss = 0.18949484\n",
      "Iteration 620, loss = 0.18894656\n",
      "Iteration 621, loss = 0.18839970\n",
      "Iteration 622, loss = 0.18785426\n",
      "Iteration 623, loss = 0.18731024\n",
      "Iteration 624, loss = 0.18676765\n",
      "Iteration 625, loss = 0.18622648\n",
      "Iteration 626, loss = 0.18568673\n",
      "Iteration 627, loss = 0.18514840\n",
      "Iteration 628, loss = 0.18461149\n",
      "Iteration 629, loss = 0.18407599\n",
      "Iteration 630, loss = 0.18354191\n",
      "Iteration 631, loss = 0.18300925\n",
      "Iteration 632, loss = 0.18247800\n",
      "Iteration 633, loss = 0.18194816\n",
      "Iteration 634, loss = 0.18141973\n",
      "Iteration 635, loss = 0.18089271\n",
      "Iteration 636, loss = 0.18036709\n",
      "Iteration 637, loss = 0.17984289\n",
      "Iteration 638, loss = 0.17932009\n",
      "Iteration 639, loss = 0.17879870\n",
      "Iteration 640, loss = 0.17827870\n",
      "Iteration 641, loss = 0.17776011\n",
      "Iteration 642, loss = 0.17724292\n",
      "Iteration 643, loss = 0.17672713\n",
      "Iteration 644, loss = 0.17621273\n",
      "Iteration 645, loss = 0.17569973\n",
      "Iteration 646, loss = 0.17518813\n",
      "Iteration 647, loss = 0.17467792\n",
      "Iteration 648, loss = 0.17416909\n",
      "Iteration 649, loss = 0.17366166\n",
      "Iteration 650, loss = 0.17315562\n",
      "Iteration 651, loss = 0.17265096\n",
      "Iteration 652, loss = 0.17214769\n",
      "Iteration 653, loss = 0.17164580\n",
      "Iteration 654, loss = 0.17114529\n",
      "Iteration 655, loss = 0.17064616\n",
      "Iteration 656, loss = 0.17014841\n",
      "Iteration 657, loss = 0.16965204\n",
      "Iteration 658, loss = 0.16915704\n",
      "Iteration 659, loss = 0.16866341\n",
      "Iteration 660, loss = 0.16817116\n",
      "Iteration 661, loss = 0.16768027\n",
      "Iteration 662, loss = 0.16719075\n",
      "Iteration 663, loss = 0.16670260\n",
      "Iteration 664, loss = 0.16621582\n",
      "Iteration 665, loss = 0.16573039\n",
      "Iteration 666, loss = 0.16524633\n",
      "Iteration 667, loss = 0.16476363\n",
      "Iteration 668, loss = 0.16428228\n",
      "Iteration 669, loss = 0.16380229\n",
      "Iteration 670, loss = 0.16332365\n",
      "Iteration 671, loss = 0.16284636\n",
      "Iteration 672, loss = 0.16237042\n",
      "Iteration 673, loss = 0.16189583\n",
      "Iteration 674, loss = 0.16142259\n",
      "Iteration 675, loss = 0.16095069\n",
      "Iteration 676, loss = 0.16048013\n",
      "Iteration 677, loss = 0.16001091\n",
      "Iteration 678, loss = 0.15954303\n",
      "Iteration 679, loss = 0.15907649\n",
      "Iteration 680, loss = 0.15861127\n",
      "Iteration 681, loss = 0.15814739\n",
      "Iteration 682, loss = 0.15768485\n",
      "Iteration 683, loss = 0.15722362\n",
      "Iteration 684, loss = 0.15676373\n",
      "Iteration 685, loss = 0.15630516\n",
      "Iteration 686, loss = 0.15584791\n",
      "Iteration 687, loss = 0.15539197\n",
      "Iteration 688, loss = 0.15493736\n",
      "Iteration 689, loss = 0.15448406\n",
      "Iteration 690, loss = 0.15403208\n",
      "Iteration 691, loss = 0.15358140\n",
      "Iteration 692, loss = 0.15313204\n",
      "Iteration 693, loss = 0.15268398\n",
      "Iteration 694, loss = 0.15223723\n",
      "Iteration 695, loss = 0.15179178\n",
      "Iteration 696, loss = 0.15134763\n",
      "Iteration 697, loss = 0.15090477\n",
      "Iteration 698, loss = 0.15046322\n",
      "Iteration 699, loss = 0.15002295\n",
      "Iteration 700, loss = 0.14958398\n",
      "Iteration 701, loss = 0.14914630\n",
      "Iteration 702, loss = 0.14870990\n",
      "Iteration 703, loss = 0.14827479\n",
      "Iteration 704, loss = 0.14784097\n",
      "Iteration 705, loss = 0.14740842\n",
      "Iteration 706, loss = 0.14697715\n",
      "Iteration 707, loss = 0.14654715\n",
      "Iteration 708, loss = 0.14611843\n",
      "Iteration 709, loss = 0.14569098\n",
      "Iteration 710, loss = 0.14526480\n",
      "Iteration 711, loss = 0.14483989\n",
      "Iteration 712, loss = 0.14441624\n",
      "Iteration 713, loss = 0.14399385\n",
      "Iteration 714, loss = 0.14357272\n",
      "Iteration 715, loss = 0.14315285\n",
      "Iteration 716, loss = 0.14273423\n",
      "Iteration 717, loss = 0.14231687\n",
      "Iteration 718, loss = 0.14190075\n",
      "Iteration 719, loss = 0.14148588\n",
      "Iteration 720, loss = 0.14107226\n",
      "Iteration 721, loss = 0.14065988\n",
      "Iteration 722, loss = 0.14024874\n",
      "Iteration 723, loss = 0.13983884\n",
      "Iteration 724, loss = 0.13943018\n",
      "Iteration 725, loss = 0.13902274\n",
      "Iteration 726, loss = 0.13861654\n",
      "Iteration 727, loss = 0.13821157\n",
      "Iteration 728, loss = 0.13780782\n",
      "Iteration 729, loss = 0.13740530\n",
      "Iteration 730, loss = 0.13700399\n",
      "Iteration 731, loss = 0.13660391\n",
      "Iteration 732, loss = 0.13620504\n",
      "Iteration 733, loss = 0.13580738\n",
      "Iteration 734, loss = 0.13541094\n",
      "Iteration 735, loss = 0.13501570\n",
      "Iteration 736, loss = 0.13462167\n",
      "Iteration 737, loss = 0.13422885\n",
      "Iteration 738, loss = 0.13383722\n",
      "Iteration 739, loss = 0.13344679\n",
      "Iteration 740, loss = 0.13305756\n",
      "Iteration 741, loss = 0.13266952\n",
      "Iteration 742, loss = 0.13228268\n",
      "Iteration 743, loss = 0.13189702\n",
      "Iteration 744, loss = 0.13151254\n",
      "Iteration 745, loss = 0.13112925\n",
      "Iteration 746, loss = 0.13074714\n",
      "Iteration 747, loss = 0.13036621\n",
      "Iteration 748, loss = 0.12998646\n",
      "Iteration 749, loss = 0.12960787\n",
      "Iteration 750, loss = 0.12923046\n",
      "Iteration 751, loss = 0.12885422\n",
      "Iteration 752, loss = 0.12847914\n",
      "Iteration 753, loss = 0.12810522\n",
      "Iteration 754, loss = 0.12773246\n",
      "Iteration 755, loss = 0.12736086\n",
      "Iteration 756, loss = 0.12699042\n",
      "Iteration 757, loss = 0.12662113\n",
      "Iteration 758, loss = 0.12625299\n",
      "Iteration 759, loss = 0.12588599\n",
      "Iteration 760, loss = 0.12552014\n",
      "Iteration 761, loss = 0.12515543\n",
      "Iteration 762, loss = 0.12479186\n",
      "Iteration 763, loss = 0.12442942\n",
      "Iteration 764, loss = 0.12406812\n",
      "Iteration 765, loss = 0.12370796\n",
      "Iteration 766, loss = 0.12334892\n",
      "Iteration 767, loss = 0.12299100\n",
      "Iteration 768, loss = 0.12263421\n",
      "Iteration 769, loss = 0.12227854\n",
      "Iteration 770, loss = 0.12192399\n",
      "Iteration 771, loss = 0.12157055\n",
      "Iteration 772, loss = 0.12121823\n",
      "Iteration 773, loss = 0.12086701\n",
      "Iteration 774, loss = 0.12051691\n",
      "Iteration 775, loss = 0.12016790\n",
      "Iteration 776, loss = 0.11982000\n",
      "Iteration 777, loss = 0.11947320\n",
      "Iteration 778, loss = 0.11912750\n",
      "Iteration 779, loss = 0.11878289\n",
      "Iteration 780, loss = 0.11843937\n",
      "Iteration 781, loss = 0.11809694\n",
      "Iteration 782, loss = 0.11775560\n",
      "Iteration 783, loss = 0.11741534\n",
      "Iteration 784, loss = 0.11707616\n",
      "Iteration 785, loss = 0.11673805\n",
      "Iteration 786, loss = 0.11640103\n",
      "Iteration 787, loss = 0.11606507\n",
      "Iteration 788, loss = 0.11573019\n",
      "Iteration 789, loss = 0.11539637\n",
      "Iteration 790, loss = 0.11506362\n",
      "Iteration 791, loss = 0.11473193\n",
      "Iteration 792, loss = 0.11440130\n",
      "Iteration 793, loss = 0.11407172\n",
      "Iteration 794, loss = 0.11374320\n",
      "Iteration 795, loss = 0.11341573\n",
      "Iteration 796, loss = 0.11308930\n",
      "Iteration 797, loss = 0.11276393\n",
      "Iteration 798, loss = 0.11243959\n",
      "Iteration 799, loss = 0.11211630\n",
      "Iteration 800, loss = 0.11179404\n",
      "Iteration 801, loss = 0.11147282\n",
      "Iteration 802, loss = 0.11115263\n",
      "Iteration 803, loss = 0.11083347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 804, loss = 0.11051533\n",
      "Iteration 805, loss = 0.11019822\n",
      "Iteration 806, loss = 0.10988213\n",
      "Iteration 807, loss = 0.10956706\n",
      "Iteration 808, loss = 0.10925301\n",
      "Iteration 809, loss = 0.10893997\n",
      "Iteration 810, loss = 0.10862793\n",
      "Iteration 811, loss = 0.10831691\n",
      "Iteration 812, loss = 0.10800689\n",
      "Iteration 813, loss = 0.10769787\n",
      "Iteration 814, loss = 0.10738986\n",
      "Iteration 815, loss = 0.10708284\n",
      "Iteration 816, loss = 0.10677681\n",
      "Iteration 817, loss = 0.10647178\n",
      "Iteration 818, loss = 0.10616773\n",
      "Iteration 819, loss = 0.10586467\n",
      "Iteration 820, loss = 0.10556259\n",
      "Iteration 821, loss = 0.10526149\n",
      "Iteration 822, loss = 0.10496138\n",
      "Iteration 823, loss = 0.10466223\n",
      "Iteration 824, loss = 0.10436406\n",
      "Iteration 825, loss = 0.10406686\n",
      "Iteration 826, loss = 0.10377063\n",
      "Iteration 827, loss = 0.10347536\n",
      "Iteration 828, loss = 0.10318105\n",
      "Iteration 829, loss = 0.10288770\n",
      "Iteration 830, loss = 0.10259530\n",
      "Iteration 831, loss = 0.10230386\n",
      "Iteration 832, loss = 0.10201338\n",
      "Iteration 833, loss = 0.10172383\n",
      "Iteration 834, loss = 0.10143524\n",
      "Iteration 835, loss = 0.10114759\n",
      "Iteration 836, loss = 0.10086087\n",
      "Iteration 837, loss = 0.10057510\n",
      "Iteration 838, loss = 0.10029026\n",
      "Iteration 839, loss = 0.10000635\n",
      "Iteration 840, loss = 0.09972337\n",
      "Iteration 841, loss = 0.09944132\n",
      "Iteration 842, loss = 0.09916019\n",
      "Iteration 843, loss = 0.09887998\n",
      "Iteration 844, loss = 0.09860070\n",
      "Iteration 845, loss = 0.09832232\n",
      "Iteration 846, loss = 0.09804486\n",
      "Iteration 847, loss = 0.09776832\n",
      "Iteration 848, loss = 0.09749268\n",
      "Iteration 849, loss = 0.09721794\n",
      "Iteration 850, loss = 0.09694411\n",
      "Iteration 851, loss = 0.09667118\n",
      "Iteration 852, loss = 0.09639914\n",
      "Iteration 853, loss = 0.09612800\n",
      "Iteration 854, loss = 0.09585775\n",
      "Iteration 855, loss = 0.09558839\n",
      "Iteration 856, loss = 0.09531992\n",
      "Iteration 857, loss = 0.09505233\n",
      "Iteration 858, loss = 0.09478563\n",
      "Iteration 859, loss = 0.09451980\n",
      "Iteration 860, loss = 0.09425485\n",
      "Iteration 861, loss = 0.09399077\n",
      "Iteration 862, loss = 0.09372756\n",
      "Iteration 863, loss = 0.09346522\n",
      "Iteration 864, loss = 0.09320375\n",
      "Iteration 865, loss = 0.09294314\n",
      "Iteration 866, loss = 0.09268339\n",
      "Iteration 867, loss = 0.09242450\n",
      "Iteration 868, loss = 0.09216646\n",
      "Iteration 869, loss = 0.09190928\n",
      "Iteration 870, loss = 0.09165295\n",
      "Iteration 871, loss = 0.09139746\n",
      "Iteration 872, loss = 0.09114282\n",
      "Iteration 873, loss = 0.09088902\n",
      "Iteration 874, loss = 0.09063606\n",
      "Iteration 875, loss = 0.09038394\n",
      "Iteration 876, loss = 0.09013265\n",
      "Iteration 877, loss = 0.08988220\n",
      "Iteration 878, loss = 0.08963257\n",
      "Iteration 879, loss = 0.08938377\n",
      "Iteration 880, loss = 0.08913579\n",
      "Iteration 881, loss = 0.08888864\n",
      "Iteration 882, loss = 0.08864230\n",
      "Iteration 883, loss = 0.08839679\n",
      "Iteration 884, loss = 0.08815208\n",
      "Iteration 885, loss = 0.08790819\n",
      "Iteration 886, loss = 0.08766511\n",
      "Iteration 887, loss = 0.08742283\n",
      "Iteration 888, loss = 0.08718136\n",
      "Iteration 889, loss = 0.08694068\n",
      "Iteration 890, loss = 0.08670081\n",
      "Iteration 891, loss = 0.08646173\n",
      "Iteration 892, loss = 0.08622345\n",
      "Iteration 893, loss = 0.08598596\n",
      "Iteration 894, loss = 0.08574925\n",
      "Iteration 895, loss = 0.08551334\n",
      "Iteration 896, loss = 0.08527820\n",
      "Iteration 897, loss = 0.08504385\n",
      "Iteration 898, loss = 0.08481028\n",
      "Iteration 899, loss = 0.08457748\n",
      "Iteration 900, loss = 0.08434545\n",
      "Iteration 901, loss = 0.08411420\n",
      "Iteration 902, loss = 0.08388372\n",
      "Iteration 903, loss = 0.08365400\n",
      "Iteration 904, loss = 0.08342505\n",
      "Iteration 905, loss = 0.08319686\n",
      "Iteration 906, loss = 0.08296942\n",
      "Iteration 907, loss = 0.08274275\n",
      "Iteration 908, loss = 0.08251682\n",
      "Iteration 909, loss = 0.08229165\n",
      "Iteration 910, loss = 0.08206723\n",
      "Iteration 911, loss = 0.08184355\n",
      "Iteration 912, loss = 0.08162062\n",
      "Iteration 913, loss = 0.08139844\n",
      "Iteration 914, loss = 0.08117699\n",
      "Iteration 915, loss = 0.08095627\n",
      "Iteration 916, loss = 0.08073630\n",
      "Iteration 917, loss = 0.08051705\n",
      "Iteration 918, loss = 0.08029853\n",
      "Iteration 919, loss = 0.08008075\n",
      "Iteration 920, loss = 0.07986368\n",
      "Iteration 921, loss = 0.07964734\n",
      "Iteration 922, loss = 0.07943172\n",
      "Iteration 923, loss = 0.07921682\n",
      "Iteration 924, loss = 0.07900263\n",
      "Iteration 925, loss = 0.07878916\n",
      "Iteration 926, loss = 0.07857640\n",
      "Iteration 927, loss = 0.07836434\n",
      "Iteration 928, loss = 0.07815299\n",
      "Iteration 929, loss = 0.07794235\n",
      "Iteration 930, loss = 0.07773241\n",
      "Iteration 931, loss = 0.07752316\n",
      "Iteration 932, loss = 0.07731462\n",
      "Iteration 933, loss = 0.07710676\n",
      "Iteration 934, loss = 0.07689960\n",
      "Iteration 935, loss = 0.07669313\n",
      "Iteration 936, loss = 0.07648735\n",
      "Iteration 937, loss = 0.07628225\n",
      "Iteration 938, loss = 0.07607784\n",
      "Iteration 939, loss = 0.07587410\n",
      "Iteration 940, loss = 0.07567105\n",
      "Iteration 941, loss = 0.07546867\n",
      "Iteration 942, loss = 0.07526696\n",
      "Iteration 943, loss = 0.07506593\n",
      "Iteration 944, loss = 0.07486556\n",
      "Iteration 945, loss = 0.07466587\n",
      "Iteration 946, loss = 0.07446683\n",
      "Iteration 947, loss = 0.07426846\n",
      "Iteration 948, loss = 0.07407075\n",
      "Iteration 949, loss = 0.07387370\n",
      "Iteration 950, loss = 0.07367730\n",
      "Iteration 951, loss = 0.07348156\n",
      "Iteration 952, loss = 0.07328647\n",
      "Iteration 953, loss = 0.07309202\n",
      "Iteration 954, loss = 0.07289823\n",
      "Iteration 955, loss = 0.07270508\n",
      "Iteration 956, loss = 0.07251257\n",
      "Iteration 957, loss = 0.07232070\n",
      "Iteration 958, loss = 0.07212947\n",
      "Iteration 959, loss = 0.07193888\n",
      "Iteration 960, loss = 0.07174892\n",
      "Iteration 961, loss = 0.07155959\n",
      "Iteration 962, loss = 0.07137089\n",
      "Iteration 963, loss = 0.07118282\n",
      "Iteration 964, loss = 0.07099537\n",
      "Iteration 965, loss = 0.07080855\n",
      "Iteration 966, loss = 0.07062235\n",
      "Iteration 967, loss = 0.07043676\n",
      "Iteration 968, loss = 0.07025180\n",
      "Iteration 969, loss = 0.07006744\n",
      "Iteration 970, loss = 0.06988370\n",
      "Iteration 971, loss = 0.06970057\n",
      "Iteration 972, loss = 0.06951805\n",
      "Iteration 973, loss = 0.06933614\n",
      "Iteration 974, loss = 0.06915482\n",
      "Iteration 975, loss = 0.06897411\n",
      "Iteration 976, loss = 0.06879400\n",
      "Iteration 977, loss = 0.06861449\n",
      "Iteration 978, loss = 0.06843558\n",
      "Iteration 979, loss = 0.06825725\n",
      "Iteration 980, loss = 0.06807952\n",
      "Iteration 981, loss = 0.06790238\n",
      "Iteration 982, loss = 0.06772582\n",
      "Iteration 983, loss = 0.06754985\n",
      "Iteration 984, loss = 0.06737447\n",
      "Iteration 985, loss = 0.06719966\n",
      "Iteration 986, loss = 0.06702544\n",
      "Iteration 987, loss = 0.06685179\n",
      "Iteration 988, loss = 0.06667871\n",
      "Iteration 989, loss = 0.06650621\n",
      "Iteration 990, loss = 0.06633429\n",
      "Iteration 991, loss = 0.06616293\n",
      "Iteration 992, loss = 0.06599213\n",
      "Iteration 993, loss = 0.06582191\n",
      "Iteration 994, loss = 0.06565224\n",
      "Iteration 995, loss = 0.06548314\n",
      "Iteration 996, loss = 0.06531460\n",
      "Iteration 997, loss = 0.06514661\n",
      "Iteration 998, loss = 0.06497918\n",
      "Iteration 999, loss = 0.06481230\n",
      "Iteration 1000, loss = 0.06464598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mahmudur Limon\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
       "              beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(50,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=10, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLPClassifier(hidden_layer_sizes=(50,) ,activation = 'logistic',solver ='adam',max_iter=1000,verbose=10)\n",
    "\n",
    "model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "predict= model.predict(X)\n",
    "\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(model.score(X,Y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
